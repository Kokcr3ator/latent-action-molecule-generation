_target_: interdiff.trainers.TrainConfig
grad_clip: 1.0
device: ${sys.device}
mixed_dtype: ${sys.dtype}         
compile_model: ${sys.compile}
always_save_checkpoint: ${sys.always_save_checkpoint}
ckpt_path: ${ckpt.path}
eval_interval: 1000 # how many steps before running eval
eval_iters: 5 # how many eval steps to average over for the computation of val loss
log_interval: 20 # number of train steps before reporting train loss
max_iters: 100000 # total number of training steps 
warmup_iters: 1000
batch_size: 512
gradient_accumulation_steps: 1 
n_mols_generate: 200 # number of molecules to generate at eval time to calculate metrics
pad_token_id: ${tokenizer.pad_token_id}
tokenizer_dir: ${tokenizer.output_dir}
dataset_path: ${data.smiles}
use_selfies: false
pad_token_id: 0
unk_token_id: 1
mask_token_id: 2
bos_token_id: 3
eos_token_id: 4
pad_token: "[PAD]"
unk_token: "[UNK]"
mask_token: "[MASK]"
bos_token: "[BOS]"
eos_token: "[EOS]"
seq_length: ${context.seq_len}
vocab_size: 500
output_dir: interdiff/data/processed/${data.name}_tok_seqlen_${context.seq_len}_vocabsize_${tokenizer.vocab_size}